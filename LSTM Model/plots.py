
"""
Various plots with data obtained from ngram and rnn runs.
"""

import matplotlib.pyplot as plt
import seaborn as sns



# --------------------------------------------------------------------------------
# Plot accuracy vs train amount for ngram and rnn
# --------------------------------------------------------------------------------

if 1:

    x      = [1e3,    1e4,    1e5,    0.5e6, 1e6]
    yngram = [0.010,  0.0527, 0.0943, 0.135, 0.187]
    yrnn   = [0.0512, 0.139,  0.155,  0.191, 0.244]

    plt.plot(x, yngram, label='n-gram')
    plt.plot(x, yrnn, label='rnn')
    plt.xlabel('Train Amount (tokens)')
    plt.ylabel('Accuracy')
    plt.title("Accuracy vs Train Amount")
    plt.xscale('log')
    plt.legend()
    plt.show()

# --------------------------------------------------------------------------------
# Plot Accuracy vs Loss
# --------------------------------------------------------------------------------

if 0:

    h = {'val_acc': [0.20527655635269615, 0.21715652241223501, 0.22394507444494349, 0.22772506364497433, 0.22973077220009275, 0.23158219547944148, 0.23227647921005939, 0.23003934274703405, 0.22957648692662211, 0.23019362802050469], 'acc': [0.1823163347843656, 0.21112129330479754, 0.22192792565345465, 0.22880324417162642, 0.23324184943982745, 0.23712251205100093, 0.2404148412747813, 0.24285155452500753, 0.24464616617040083, 0.24643999856627377], 'val_loss': [4.8049610608854314, 4.7123360224990654, 4.6866933162276929, 4.6646330226748702, 4.6497976859392214, 4.6567215982580006, 4.6583729048326301, 4.6813602669504073, 4.6759254413190687, 4.6896494783057037], 'loss': [5.1152711713223376, 4.7713241221684806, 4.6645022095402258, 4.6001300744891491, 4.5553911364604689, 4.5207795701743079, 4.4964553604509918, 4.4763038436968046, 4.4601910206918296, 4.4465202019751304]}

    plt.scatter(h['val_loss'], h['val_acc'])
    plt.xlabel('loss')
    plt.ylabel('accuracy')
    plt.title("Accuracy vs Loss")
    plt.show()

# --------------------------------------------------------------------------------
# Plot Multiple Loss Curves
# --------------------------------------------------------------------------------

if 0:

    hadam = {'val_acc': [0.20527655635269615, 0.21715652241223501, 0.22394507444494349, 0.22772506364497433, 0.22973077220009275, 0.23158219547944148, 0.23227647921005939, 0.23003934274703405, 0.22957648692662211, 0.23019362802050469], 'acc': [0.1823163347843656, 0.21112129330479754, 0.22192792565345465, 0.22880324417162642, 0.23324184943982745, 0.23712251205100093, 0.2404148412747813, 0.24285155452500753, 0.24464616617040083, 0.24643999856627377], 'val_loss': [4.8049610608854314, 4.7123360224990654, 4.6866933162276929, 4.6646330226748702, 4.6497976859392214, 4.6567215982580006, 4.6583729048326301, 4.6813602669504073, 4.6759254413190687, 4.6896494783057037], 'loss': [5.1152711713223376, 4.7713241221684806, 4.6645022095402258, 4.6001300744891491, 4.5553911364604689, 4.5207795701743079, 4.4964553604509918, 4.4763038436968046, 4.4601910206918296, 4.4465202019751304]}

    hsgd = {'acc': [0.11584868844515549, 0.1352161560244792, 0.143065536443232, 0.15106219502131185, 0.15710683355079039, 0.16162726001832856, 0.16489153625937555, 0.16728850778401635, 0.1694322232145595, 0.17189465169891155], 'val_loss': [5.7668266812472915, 5.605574571322137, 5.4865395793813274, 5.4045470141062335, 5.3335361192322601, 5.2814175137433983, 5.229823076622929, 5.1870213716217535, 5.1492352545983282, 5.1227073606899989], 'loss': [6.0595294126164072, 5.7096473499757616, 5.5784740443063532, 5.4778623018776607, 5.4022782620568686, 5.3403315211338303, 5.2896896385438215, 5.2457373058366779, 5.2060566848121734, 5.1699000555296388], 'val_acc': [0.12851963280104914, 0.14001388567461237, 0.14772814934814471, 0.15451670138085319, 0.15845097585435469, 0.16053382704620844, 0.16338810460541542, 0.1650080999768572, 0.17125665355241843, 0.17033094191159454]}

    hrmsprop = {'acc': [0.16871687215481163, 0.18569048520759926, 0.19068703313217755, 0.19389364490850799, 0.19637165838336038, 0.19804860335122398, 0.19963437612515761, 0.20054765656296383, 0.20150379572452984, 0.20238123068439895], 'val_acc': [0.18136233896474582, 0.18205662269536374, 0.18938517318521947, 0.19031088482604336, 0.18869088945460155, 0.19254802129136775, 0.19524801357710406, 0.19038802746277866, 0.19239373601789708, 0.19463087248322147], 'loss': [6.469419267194775, 6.9324817610592788, 6.9294091759793588, 6.9137105880923322, 6.9044213481848038, 6.8982559030043884, 6.8910966860508633, 6.8874106410457685, 6.8822540717162743, 6.8785967750945325], 'val_loss': [6.8474436628131565, 6.8796323025326833, 6.8623272938501376, 6.8588246311071357, 6.8547166805801441, 6.8496172390702901, 6.8491008653690777, 6.8429889798964432, 6.8432240480627344, 6.8436115066421941]}

    hadagrad = {'acc': [0.1561725133759109, 0.17437422367278144, 0.18075238099704885, 0.18441173674466893, 0.18818953841943201, 0.19038234656978181, 0.19283932030749124, 0.19451548602585769, 0.19627892769041291, 0.19798860113808567], 'val_loss': [5.1484461337578553, 5.015860971197581, 4.945420968590498, 4.8960460622231734, 4.8612829415985601, 4.8315919795393496, 4.8082322252207792, 4.7880345565397446, 4.7705184664980438, 4.7554069982674498], 'val_acc': [0.16817094808300548, 0.1764252102136851, 0.17897091722595079, 0.18267376378924632, 0.18460232970762941, 0.1856051839851886, 0.1891537452750135, 0.19061945537298464, 0.19247087865463242, 0.19470801511995681], 'loss': [5.4000936325396447, 5.086119159794781, 4.9796651922503443, 4.9126110203507372, 4.8635725267661618, 4.8254848455971162, 4.7935531300068872, 4.7665741205502679, 4.743638134088445, 4.7229959474016647]}

    # plot loss vs epoch
    plt.plot(hadam['val_loss'], label='adam')
    plt.plot(hsgd['val_loss'], label='sgd')
    plt.plot(hrmsprop['val_loss'], label='rmsprop')
    plt.plot(hadagrad['val_loss'], label='adagrad')
    plt.xlabel('epoch-1')
    plt.ylabel('loss')
    plt.title("Optimizers: Validation Loss vs Epoch")
    plt.legend()
    plt.show()

    # plot accuracy vs epoch
    plt.plot(hadam['val_acc'], label='adam')
    plt.plot(hsgd['val_acc'], label='sgd')
    plt.plot(hrmsprop['val_acc'], label='rmsprop')
    plt.plot(hadagrad['val_acc'], label='adagrad')
    plt.xlabel('epoch-1')
    plt.ylabel('accuracy')
    plt.title("Optimizers: Validation Accuracy vs Epoch")
    plt.legend()
    plt.show()


# --------------------------------------------------------------------------------
# Plot Single Loss Curves
# --------------------------------------------------------------------------------

if 0:

    h = {'acc': [0.15588981766413257, 0.17354520525744341, 0.17787158173789691, 0.18073219558155143, 0.18200626358325769, 0.18300993305495195, 0.18343618088184574, 0.18427387085223546, 0.18486609818040475, 0.18524714971104456], 'loss': [5.3683105373570656, 5.0982991809959106, 5.0393321289147348, 5.0104709816209976, 5.0010590197910627, 4.9961239868211402, 4.9928397168714769, 4.9917874247343672, 4.9930715708899731, 4.9969904191791672], 'val_loss': [5.0744118071408, 4.9902474920865352, 4.9647443818887851, 4.9567390965852605, 4.934373090510511, 4.9218365671751831, 4.925168130883903, 4.9363245936461935, 4.9185292135001903, 4.9278917894920324], 'val_acc': [0.17287664892615925, 0.18082234050989757, 0.18367661806910454, 0.18383090334257521, 0.18637661035484088, 0.18722517935892943, 0.18938517318751849, 0.18892231736710655, 0.19223945074672547, 0.19254802129366677]}

    # plot loss vs epoch
    plt.plot(h['loss'], label='Training')
    plt.plot(h['val_loss'], label='Validation')
    plt.xlabel('epoch-1')
    plt.ylabel('loss')
    plt.title("Training and Validation Loss vs Epoch")
    plt.legend()
    plt.show()

    # plot accuracy vs epoch
    plt.plot(h['acc'], label='Training')
    plt.plot(h['val_acc'], label='Validation')
    plt.xlabel('epoch-1')
    plt.ylabel('accuracy')
    plt.title("Training and Validation Accuracy vs Epoch")
    plt.legend()
    plt.show()


# --------------------------------------------------------------------------------
# Visualize Embeddings
# --------------------------------------------------------------------------------

if 0:

    from sklearn.decomposition import PCA

    words = 'alice rabbit mouse said was fell small white gray'.split()
    print('words',words)
    iwords = [data.word_to_iword[word] for word in words]
    print('iwords',iwords)
    vecs = [E[iword] for iword in iwords]
    print('word embedding for alice',vecs[1])

    # now want to reduce dims of these vectors
    pca = PCA(n_components=2)
    pca.fit(vecs)
    vecnew = pca.transform(vecs)
    print('some projections',vecnew[:3])

    # now plot the new vectors with labels
    x = [vec[0] for vec in vecnew]
    y = [vec[1] for vec in vecnew]
    plt.scatter(x, y)

    for i, word in enumerate(words):
        plt.annotate(word, (x[i]+0.1,y[i]+0.1))

    plt.title("Word embeddings projected to 2D")
    plt.xlabel("PCA 1")
    plt.ylabel("PCA 2")
    plt.show()



