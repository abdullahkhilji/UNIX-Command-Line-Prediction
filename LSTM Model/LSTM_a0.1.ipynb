{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing libraries (~10sec)...\n",
      "3.6.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:42:37) \n",
      "[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/abdullahkhilji/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "print('Importing libraries (~10sec)...')\n",
    "\n",
    "import sys\n",
    "print(sys.version)\n",
    "import os\n",
    "import os.path\n",
    "import random\n",
    "import re\n",
    "import heapq\n",
    "from importlib import reload\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk import tokenize\n",
    "\n",
    "#from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import SimpleRNN, LSTM, GRU\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "from keras.metrics import top_k_categorical_accuracy\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "except:\n",
    "    plt = None\n",
    "\n",
    "# local modules\n",
    "import data as datamodule\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug            = 0         # 0 or 1\n",
    "DATASET          = 'gutenbergs' # dataset to use (gutenbergs, alice1)\n",
    "TRAIN_AMOUNT     = 0.9      # percent of training data to use (for debugging), 0.0 to 1.0\n",
    "NEPOCHS          = 100         # number of epochs to train model\n",
    "LAYERS           = 3         # number of RNN layers, 1 to 3\n",
    "DROPOUT          = 0.1       # amount of dropout to apply after each layer, 0.0 to 1.0\n",
    "NVOCAB           = 715   # number of vocabulary words to use\n",
    "EMBEDDING_DIM    = 50       # dimension of embedding layer - 50, 100, 200, 300\n",
    "TRAINABLE        = True      # allow embedding matrix to be trained?\n",
    "NHIDDEN1        = 200# size of hidden layer(s)\n",
    "NHIDDEN2        = 100\n",
    "NHIDDEN         = 100\n",
    "N            = 10        # amount to unfold recurrent network\n",
    "RNN_CLASS        = GRU       # type of RNN to use - SimpleRNN, LSTM, or GRU\n",
    "BATCH_SIZE       = 20000        # size of batch to use for training\n",
    "INITIAL_EPOCH    = 0         # to continue training\n",
    "PATIENCE         = 10        # stop after this many epochs of no improvement\n",
    "VALIDATION_SPLIT = 0.01      # percent of training data to use for validation (0.01 ~10k tokens)\n",
    "NTEST            = 10000     # number of tokens to use for testing\n",
    "OPTIMIZER        = 'adam'    # optimizing algorithm to use (sgd, rmsprop, adam, adagrad, adadelta, adamax, nadam)\n",
    "INITIALIZER      = 'uniform' # random weight initializer (uniform, normal, lecun_uniform, glorot_uniform [default])\n",
    "SEED             = 0         # random number seed\n",
    "\n",
    "# LOSS_FN    = 'categorical_crossentropy' # allows calculation of top_k_accuracy, but requires one-hot encoding y values\n",
    "LOSS_FN    = 'sparse_categorical_crossentropy'\n",
    "BASE_DIR   = '.'\n",
    "GLOVE_DIR  = BASE_DIR + '/_vectors/glove.6B'\n",
    "GLOVE_FILE = '/Users/abdullahkhilji/Extras/DeepDive/Final to check/tf/Word-Prediction-with-LSTM/vectors/glove.6B/vectors.txt '\n",
    "MODEL_DIR  = BASE_DIR + '/_models/' + DATASET\n",
    "MODEL_FILE = MODEL_DIR + \"/model-train_amount-%s-nvocab-%d-embedding_dim-%d-nhidden-%d-n-%d.h5\" % \\\n",
    "                         (TRAIN_AMOUNT, NVOCAB, EMBEDDING_DIM, NHIDDEN, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import util\n",
    "data = datamodule.Data(DATASET)\n",
    "data.folder = '/Users/abdullahkhilji/Extras/DeepDive/Final to check/tf/Word-Prediction-with-LSTM/data/' + data.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading texts...\n",
      "/Users/abdullahkhilji/Extras/DeepDive/Final to check/tf/Word-Prediction-with-LSTM/data/gutenbergs/testfile.txt\n",
      "Split into paragraphs, shuffle, recombine...\n",
      "Tokenizing text (~15sec)...\n",
      "Find vocabulary words...\n",
      "Convert text to numeric sequence, skipping OOV words...\n"
     ]
    }
   ],
   "source": [
    "import data as datamodule\n",
    "data.prepare(nvocab=NVOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create train and test sets...\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = data.split(n=N, ntest=NTEST,\n",
    "                                              train_amount=TRAIN_AMOUNT, debug=debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169171"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading word vectors (~15sec)...\n"
     ]
    }
   ],
   "source": [
    "print('Reading word vectors (~15sec)...')\n",
    "word_vectors = {}\n",
    "with open('/Users/abdullahkhilji/Extras/DeepDive/Final to check/tf/Word-Prediction-with-LSTM/vectors/a0.1', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        word_vectors[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_matrix(data, word_vectors, nvocab):\n",
    "    nwords = nvocab\n",
    "    embedding_dim = len(word_vectors['a'])\n",
    "    E = np.zeros((nwords + 1, embedding_dim))\n",
    "    for word, iword in data.word_to_iword.items():\n",
    "        if iword > nvocab:\n",
    "            continue\n",
    "        word_vector = word_vectors.get(word)\n",
    "        # words not found in embedding index will be all zeros\n",
    "        if word_vector is not None:\n",
    "            E[iword] = word_vector\n",
    "    return E\n",
    "E = get_embedding_matrix(data, word_vectors, NVOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of word vectors in matrix E 716\n",
      "example word vector: [-0.0343038   0.30809    -0.0266542  -0.91010702 -0.31334999  0.0252021\n",
      "  0.81171     0.51103002  0.26535901  0.0704833  -0.024386   -0.31169701\n",
      " -0.31038001  0.59833902 -0.24935099  0.454761    0.345485   -0.546965\n",
      "  0.515136    0.56245202  0.50753498  0.065713    0.75871301  0.52148998\n",
      " -0.16196799 -0.57184201 -0.19863699 -0.73529899 -0.23457199 -0.25543699\n",
      "  0.742603    0.49605101 -0.67958701  0.358735   -0.47995099 -0.37514001\n",
      "  0.58205098 -0.476145    0.0480055  -0.67645198 -0.407617   -0.44757599\n",
      "  0.509642   -0.661264   -0.62075698 -0.463929   -0.69187403 -0.71164799\n",
      " -0.63205701  0.241734  ]\n"
     ]
    }
   ],
   "source": [
    "print('number of word vectors in matrix E',len(E))\n",
    "print('example word vector:',E[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 167479 samples, validate on 1692 samples\n",
      "Epoch 1/100\n",
      "167479/167479 [==============================] - 69s 411us/step - loss: 5.6652 - acc: 0.2479 - val_loss: 3.3879 - val_acc: 0.2677\n",
      "Epoch 2/100\n",
      "167479/167479 [==============================] - 67s 397us/step - loss: 3.5465 - acc: 0.2853 - val_loss: 3.3422 - val_acc: 0.2677\n",
      "Epoch 3/100\n",
      "167479/167479 [==============================] - 68s 408us/step - loss: 3.4049 - acc: 0.2878 - val_loss: 3.2308 - val_acc: 0.2677\n",
      "Epoch 4/100\n",
      "167479/167479 [==============================] - 66s 393us/step - loss: 3.3458 - acc: 0.2878 - val_loss: 3.2342 - val_acc: 0.2677\n",
      "Epoch 5/100\n",
      "167479/167479 [==============================] - 66s 392us/step - loss: 3.3256 - acc: 0.2878 - val_loss: 3.2056 - val_acc: 0.2677\n",
      "Epoch 6/100\n",
      "167479/167479 [==============================] - 66s 394us/step - loss: 3.3155 - acc: 0.2878 - val_loss: 3.2046 - val_acc: 0.2677\n",
      "Epoch 7/100\n",
      "167479/167479 [==============================] - 72s 428us/step - loss: 3.3091 - acc: 0.2878 - val_loss: 3.1908 - val_acc: 0.2677\n",
      "Epoch 8/100\n",
      "167479/167479 [==============================] - 68s 404us/step - loss: 3.3027 - acc: 0.2879 - val_loss: 3.1931 - val_acc: 0.2677\n",
      "Epoch 9/100\n",
      "167479/167479 [==============================] - 68s 406us/step - loss: 3.2955 - acc: 0.2889 - val_loss: 3.1960 - val_acc: 0.2677\n",
      "Epoch 10/100\n",
      "167479/167479 [==============================] - 72s 429us/step - loss: 3.2859 - acc: 0.2906 - val_loss: 3.1719 - val_acc: 0.2677\n",
      "Epoch 11/100\n",
      "167479/167479 [==============================] - 66s 395us/step - loss: 3.2714 - acc: 0.2906 - val_loss: 3.1693 - val_acc: 0.2677\n",
      "Epoch 12/100\n",
      "167479/167479 [==============================] - 71s 425us/step - loss: 3.2490 - acc: 0.2907 - val_loss: 3.1336 - val_acc: 0.2677\n",
      "Epoch 13/100\n",
      "167479/167479 [==============================] - 66s 396us/step - loss: 3.2132 - acc: 0.2910 - val_loss: 3.0925 - val_acc: 0.2677\n",
      "Epoch 14/100\n",
      "167479/167479 [==============================] - 67s 403us/step - loss: 3.1582 - acc: 0.2958 - val_loss: 3.0259 - val_acc: 0.3126\n",
      "Epoch 15/100\n",
      "167479/167479 [==============================] - 66s 396us/step - loss: 3.0880 - acc: 0.3148 - val_loss: 2.9512 - val_acc: 0.3327\n",
      "Epoch 16/100\n",
      "167479/167479 [==============================] - 66s 397us/step - loss: 3.0239 - acc: 0.3275 - val_loss: 2.8788 - val_acc: 0.3481\n",
      "Epoch 17/100\n",
      "167479/167479 [==============================] - 68s 408us/step - loss: 2.9552 - acc: 0.3473 - val_loss: 2.8091 - val_acc: 0.3676\n",
      "Epoch 18/100\n",
      "167479/167479 [==============================] - 66s 393us/step - loss: 2.8835 - acc: 0.3709 - val_loss: 2.7424 - val_acc: 0.3889\n",
      "Epoch 19/100\n",
      "167479/167479 [==============================] - 66s 393us/step - loss: 2.8079 - acc: 0.3887 - val_loss: 2.6824 - val_acc: 0.4232\n",
      "Epoch 20/100\n",
      "167479/167479 [==============================] - 66s 395us/step - loss: 2.7315 - acc: 0.4032 - val_loss: 2.6205 - val_acc: 0.4557\n",
      "Epoch 21/100\n",
      "167479/167479 [==============================] - 66s 396us/step - loss: 2.6590 - acc: 0.4175 - val_loss: 2.5468 - val_acc: 0.4976\n",
      "Epoch 22/100\n",
      "167479/167479 [==============================] - 68s 406us/step - loss: 2.5958 - acc: 0.4294 - val_loss: 2.5028 - val_acc: 0.5071\n",
      "Epoch 23/100\n",
      "167479/167479 [==============================] - 66s 394us/step - loss: 2.5436 - acc: 0.4377 - val_loss: 2.4498 - val_acc: 0.5236\n",
      "Epoch 24/100\n",
      "167479/167479 [==============================] - 68s 404us/step - loss: 2.5012 - acc: 0.4434 - val_loss: 2.4305 - val_acc: 0.5207\n",
      "Epoch 25/100\n",
      "167479/167479 [==============================] - 67s 398us/step - loss: 2.4680 - acc: 0.4485 - val_loss: 2.4049 - val_acc: 0.5349\n",
      "Epoch 26/100\n",
      "167479/167479 [==============================] - 66s 394us/step - loss: 2.4348 - acc: 0.4546 - val_loss: 2.3699 - val_acc: 0.5443\n",
      "Epoch 27/100\n",
      "167479/167479 [==============================] - 68s 406us/step - loss: 2.4034 - acc: 0.4599 - val_loss: 2.3409 - val_acc: 0.5461\n",
      "Epoch 28/100\n",
      "167479/167479 [==============================] - 66s 394us/step - loss: 2.3745 - acc: 0.4656 - val_loss: 2.3221 - val_acc: 0.5431\n",
      "Epoch 29/100\n",
      "167479/167479 [==============================] - 66s 394us/step - loss: 2.3501 - acc: 0.4693 - val_loss: 2.3020 - val_acc: 0.5414\n",
      "Epoch 30/100\n",
      "167479/167479 [==============================] - 66s 396us/step - loss: 2.3239 - acc: 0.4735 - val_loss: 2.2962 - val_acc: 0.5414\n",
      "Epoch 31/100\n",
      "167479/167479 [==============================] - 66s 394us/step - loss: 2.2990 - acc: 0.4775 - val_loss: 2.2778 - val_acc: 0.5426\n",
      "Epoch 32/100\n",
      "167479/167479 [==============================] - 68s 406us/step - loss: 2.2771 - acc: 0.4814 - val_loss: 2.2598 - val_acc: 0.5426\n",
      "Epoch 33/100\n",
      "167479/167479 [==============================] - 66s 395us/step - loss: 2.2550 - acc: 0.4848 - val_loss: 2.2310 - val_acc: 0.5461\n",
      "Epoch 34/100\n",
      "167479/167479 [==============================] - 67s 399us/step - loss: 2.2346 - acc: 0.4884 - val_loss: 2.2111 - val_acc: 0.5479\n",
      "Epoch 35/100\n",
      "167479/167479 [==============================] - 67s 398us/step - loss: 2.2152 - acc: 0.4912 - val_loss: 2.2060 - val_acc: 0.5496\n",
      "Epoch 36/100\n",
      "167479/167479 [==============================] - 66s 394us/step - loss: 2.1978 - acc: 0.4931 - val_loss: 2.1816 - val_acc: 0.5532\n",
      "Epoch 37/100\n",
      "167479/167479 [==============================] - 68s 406us/step - loss: 2.1800 - acc: 0.4959 - val_loss: 2.1646 - val_acc: 0.5591\n",
      "Epoch 38/100\n",
      "167479/167479 [==============================] - 66s 397us/step - loss: 2.1624 - acc: 0.4981 - val_loss: 2.1522 - val_acc: 0.5579\n",
      "Epoch 39/100\n",
      "167479/167479 [==============================] - 66s 397us/step - loss: 2.1444 - acc: 0.4997 - val_loss: 2.1412 - val_acc: 0.5621\n",
      "Epoch 40/100\n",
      "167479/167479 [==============================] - 67s 397us/step - loss: 2.1310 - acc: 0.5011 - val_loss: 2.1254 - val_acc: 0.5638\n",
      "Epoch 41/100\n",
      "167479/167479 [==============================] - 67s 398us/step - loss: 2.1167 - acc: 0.5033 - val_loss: 2.1205 - val_acc: 0.5691\n",
      "Epoch 42/100\n",
      "167479/167479 [==============================] - 68s 408us/step - loss: 2.1041 - acc: 0.5046 - val_loss: 2.1129 - val_acc: 0.5609\n",
      "Epoch 43/100\n",
      "167479/167479 [==============================] - 66s 396us/step - loss: 2.0915 - acc: 0.5059 - val_loss: 2.0968 - val_acc: 0.5662\n",
      "Epoch 44/100\n",
      "167479/167479 [==============================] - 66s 395us/step - loss: 2.0803 - acc: 0.5070 - val_loss: 2.0748 - val_acc: 0.5674\n",
      "Epoch 45/100\n",
      "167479/167479 [==============================] - 66s 394us/step - loss: 2.0700 - acc: 0.5083 - val_loss: 2.0891 - val_acc: 0.5727\n",
      "Epoch 46/100\n",
      "167479/167479 [==============================] - 66s 393us/step - loss: 2.0607 - acc: 0.5092 - val_loss: 2.0658 - val_acc: 0.5674\n",
      "Epoch 47/100\n",
      "167479/167479 [==============================] - 68s 408us/step - loss: 2.0510 - acc: 0.5103 - val_loss: 2.0515 - val_acc: 0.5733\n",
      "Epoch 48/100\n",
      "167479/167479 [==============================] - 66s 394us/step - loss: 2.0421 - acc: 0.5111 - val_loss: 2.0525 - val_acc: 0.5697\n",
      "Epoch 49/100\n",
      "167479/167479 [==============================] - 66s 395us/step - loss: 2.0309 - acc: 0.5128 - val_loss: 2.0480 - val_acc: 0.5691\n",
      "Epoch 50/100\n",
      "167479/167479 [==============================] - 66s 396us/step - loss: 2.0254 - acc: 0.5131 - val_loss: 2.0293 - val_acc: 0.5733\n",
      "Epoch 51/100\n",
      "167479/167479 [==============================] - 66s 395us/step - loss: 2.0193 - acc: 0.5148 - val_loss: 2.0119 - val_acc: 0.5780\n",
      "Epoch 52/100\n",
      "167479/167479 [==============================] - 68s 408us/step - loss: 2.0072 - acc: 0.5158 - val_loss: 2.0294 - val_acc: 0.5733\n",
      "Epoch 53/100\n",
      "167479/167479 [==============================] - 66s 394us/step - loss: 1.9981 - acc: 0.5171 - val_loss: 2.0121 - val_acc: 0.5780\n",
      "Epoch 54/100\n",
      "167479/167479 [==============================] - 66s 395us/step - loss: 1.9914 - acc: 0.5176 - val_loss: 2.0239 - val_acc: 0.5715\n",
      "Epoch 55/100\n",
      "167479/167479 [==============================] - 66s 395us/step - loss: 1.9838 - acc: 0.5193 - val_loss: 2.0044 - val_acc: 0.5703\n",
      "Epoch 56/100\n",
      "167479/167479 [==============================] - 66s 394us/step - loss: 1.9776 - acc: 0.5197 - val_loss: 1.9940 - val_acc: 0.5757\n",
      "Epoch 57/100\n",
      "167479/167479 [==============================] - 69s 412us/step - loss: 1.9722 - acc: 0.5198 - val_loss: 1.9983 - val_acc: 0.5739\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167479/167479 [==============================] - 66s 395us/step - loss: 1.9651 - acc: 0.5207 - val_loss: 1.9896 - val_acc: 0.5745\n",
      "Epoch 59/100\n",
      "167479/167479 [==============================] - 66s 395us/step - loss: 1.9599 - acc: 0.5214 - val_loss: 2.0065 - val_acc: 0.5691\n",
      "Epoch 60/100\n",
      "167479/167479 [==============================] - 67s 398us/step - loss: 1.9547 - acc: 0.5220 - val_loss: 1.9788 - val_acc: 0.5745\n",
      "Epoch 61/100\n",
      "167479/167479 [==============================] - 67s 398us/step - loss: 1.9483 - acc: 0.5228 - val_loss: 1.9571 - val_acc: 0.5745\n",
      "Epoch 62/100\n",
      "167479/167479 [==============================] - 68s 409us/step - loss: 1.9440 - acc: 0.5234 - val_loss: 1.9637 - val_acc: 0.5745\n",
      "Epoch 63/100\n",
      "167479/167479 [==============================] - 66s 395us/step - loss: 1.9373 - acc: 0.5248 - val_loss: 1.9816 - val_acc: 0.5697\n",
      "Epoch 64/100\n",
      "167479/167479 [==============================] - 67s 397us/step - loss: 1.9308 - acc: 0.5259 - val_loss: 1.9721 - val_acc: 0.5721\n",
      "Epoch 65/100\n",
      "167479/167479 [==============================] - 68s 407us/step - loss: 1.9243 - acc: 0.5258 - val_loss: 1.9646 - val_acc: 0.5745\n",
      "Epoch 66/100\n",
      "167479/167479 [==============================] - 70s 419us/step - loss: 1.9198 - acc: 0.5266 - val_loss: 1.9299 - val_acc: 0.5757\n",
      "Epoch 67/100\n",
      "167479/167479 [==============================] - 69s 410us/step - loss: 1.9180 - acc: 0.5275 - val_loss: 1.9480 - val_acc: 0.5668\n",
      "Epoch 68/100\n",
      "167479/167479 [==============================] - 67s 399us/step - loss: 1.9121 - acc: 0.5277 - val_loss: 1.9665 - val_acc: 0.5697\n",
      "Epoch 69/100\n",
      "167479/167479 [==============================] - 69s 409us/step - loss: 1.9077 - acc: 0.5280 - val_loss: 1.9455 - val_acc: 0.5751\n",
      "Epoch 70/100\n",
      "167479/167479 [==============================] - 68s 407us/step - loss: 1.9028 - acc: 0.5282 - val_loss: 1.9337 - val_acc: 0.5691\n",
      "Epoch 71/100\n",
      "167479/167479 [==============================] - 67s 403us/step - loss: 1.8967 - acc: 0.5293 - val_loss: 1.9366 - val_acc: 0.5727\n",
      "Epoch 72/100\n",
      "167479/167479 [==============================] - 67s 402us/step - loss: 1.8901 - acc: 0.5301 - val_loss: 1.9412 - val_acc: 0.5757\n",
      "Epoch 73/100\n",
      "167479/167479 [==============================] - 67s 400us/step - loss: 1.8862 - acc: 0.5308 - val_loss: 1.9427 - val_acc: 0.5721\n",
      "Epoch 74/100\n",
      "167479/167479 [==============================] - 66s 396us/step - loss: 1.8821 - acc: 0.5309 - val_loss: 1.9423 - val_acc: 0.5757\n",
      "Epoch 75/100\n",
      "167479/167479 [==============================] - 67s 403us/step - loss: 1.8775 - acc: 0.5321 - val_loss: 1.9575 - val_acc: 0.5680\n",
      "Epoch 76/100\n",
      "167479/167479 [==============================] - 68s 407us/step - loss: 1.8729 - acc: 0.5325 - val_loss: 1.9348 - val_acc: 0.5721\n",
      "Epoch 77/100\n",
      "167479/167479 [==============================] - 68s 409us/step - loss: 1.8675 - acc: 0.5341 - val_loss: 1.9363 - val_acc: 0.5680\n",
      "Epoch 78/100\n",
      "167479/167479 [==============================] - 69s 410us/step - loss: 1.8632 - acc: 0.5342 - val_loss: 1.9330 - val_acc: 0.5709\n",
      "Epoch 79/100\n",
      "167479/167479 [==============================] - 70s 417us/step - loss: 1.8578 - acc: 0.5354 - val_loss: 1.9138 - val_acc: 0.5751\n",
      "Epoch 80/100\n",
      "167479/167479 [==============================] - 69s 409us/step - loss: 1.8556 - acc: 0.5357 - val_loss: 1.9166 - val_acc: 0.5757\n",
      "Epoch 81/100\n",
      "167479/167479 [==============================] - 71s 423us/step - loss: 1.8501 - acc: 0.5359 - val_loss: 1.9265 - val_acc: 0.5715\n",
      "Epoch 82/100\n",
      "167479/167479 [==============================] - 78s 463us/step - loss: 1.8458 - acc: 0.5363 - val_loss: 1.9154 - val_acc: 0.5697\n",
      "Epoch 83/100\n",
      "167479/167479 [==============================] - 72s 430us/step - loss: 1.8418 - acc: 0.5373 - val_loss: 1.9065 - val_acc: 0.5757\n",
      "Epoch 84/100\n",
      "167479/167479 [==============================] - 67s 397us/step - loss: 1.8380 - acc: 0.5376 - val_loss: 1.9151 - val_acc: 0.5733\n",
      "Epoch 85/100\n",
      "167479/167479 [==============================] - 66s 395us/step - loss: 1.8341 - acc: 0.5380 - val_loss: 1.9103 - val_acc: 0.5751\n",
      "Epoch 86/100\n",
      "167479/167479 [==============================] - 68s 408us/step - loss: 1.8312 - acc: 0.5382 - val_loss: 1.9073 - val_acc: 0.5668\n",
      "Epoch 87/100\n",
      "167479/167479 [==============================] - 66s 397us/step - loss: 1.8268 - acc: 0.5401 - val_loss: 1.9161 - val_acc: 0.5697\n",
      "Epoch 88/100\n",
      "167479/167479 [==============================] - 67s 398us/step - loss: 1.8225 - acc: 0.5400 - val_loss: 1.9185 - val_acc: 0.5703\n",
      "Epoch 89/100\n",
      "167479/167479 [==============================] - 66s 395us/step - loss: 1.8197 - acc: 0.5410 - val_loss: 1.9102 - val_acc: 0.5715\n",
      "Epoch 90/100\n",
      "167479/167479 [==============================] - 66s 396us/step - loss: 1.8167 - acc: 0.5411 - val_loss: 1.9117 - val_acc: 0.5715\n",
      "Epoch 91/100\n",
      "167479/167479 [==============================] - 68s 407us/step - loss: 1.8113 - acc: 0.5419 - val_loss: 1.9127 - val_acc: 0.5691\n",
      "Epoch 92/100\n",
      "167479/167479 [==============================] - 67s 400us/step - loss: 1.8075 - acc: 0.5427 - val_loss: 1.9176 - val_acc: 0.5638\n",
      "Epoch 93/100\n",
      "167479/167479 [==============================] - 66s 396us/step - loss: 1.8035 - acc: 0.5428 - val_loss: 1.9048 - val_acc: 0.5709\n",
      "Epoch 94/100\n",
      "167479/167479 [==============================] - 68s 404us/step - loss: 1.7996 - acc: 0.5437 - val_loss: 1.8968 - val_acc: 0.5709\n",
      "Epoch 95/100\n",
      "167479/167479 [==============================] - 67s 402us/step - loss: 1.7953 - acc: 0.5442 - val_loss: 1.8935 - val_acc: 0.5680\n",
      "Epoch 96/100\n",
      "167479/167479 [==============================] - 70s 417us/step - loss: 1.7917 - acc: 0.5450 - val_loss: 1.8999 - val_acc: 0.5715\n",
      "Epoch 97/100\n",
      "167479/167479 [==============================] - 67s 403us/step - loss: 1.7879 - acc: 0.5457 - val_loss: 1.9038 - val_acc: 0.5703\n",
      "Epoch 98/100\n",
      "167479/167479 [==============================] - 67s 402us/step - loss: 1.7843 - acc: 0.5465 - val_loss: 1.8957 - val_acc: 0.5715\n",
      "Epoch 99/100\n",
      "167479/167479 [==============================] - 67s 400us/step - loss: 1.7815 - acc: 0.5472 - val_loss: 1.8913 - val_acc: 0.5757\n",
      "Epoch 100/100\n",
      "167479/167479 [==============================] - 67s 403us/step - loss: 1.7764 - acc: 0.5475 - val_loss: 1.8997 - val_acc: 0.5715\n",
      "Final epoch generated text: average parameter more parameter average sfn parameter cd cd filename cd ls parameter cd ls cd filename cd filename emacs\n",
      "\n",
      "history\n",
      "{'val_loss': [3.387859344482422, 3.3422205448150635, 3.2308201789855957, 3.234215497970581, 3.2056162357330322, 3.2045505046844482, 3.1907529830932617, 3.1930832862854004, 3.1960220336914062, 3.1718790531158447, 3.1693482398986816, 3.133573293685913, 3.092485189437866, 3.0259218215942383, 2.951216697692871, 2.8788154125213623, 2.8090834617614746, 2.7424261569976807, 2.6823678016662598, 2.620460271835327, 2.5467777252197266, 2.502843141555786, 2.4497687816619873, 2.4305484294891357, 2.4048922061920166, 2.369900703430176, 2.340853691101074, 2.322087526321411, 2.301966428756714, 2.2962374687194824, 2.277759552001953, 2.259793996810913, 2.230987310409546, 2.211055278778076, 2.20603084564209, 2.1816112995147705, 2.164585590362549, 2.152207612991333, 2.1411943435668945, 2.1254453659057617, 2.1205360889434814, 2.1128733158111572, 2.0967702865600586, 2.074843168258667, 2.0891032218933105, 2.0658226013183594, 2.051539897918701, 2.052487373352051, 2.04799747467041, 2.0293097496032715, 2.0118603706359863, 2.029365062713623, 2.0121147632598877, 2.02390718460083, 2.004389524459839, 1.9939690828323364, 1.998288869857788, 1.9895775318145752, 2.006483316421509, 1.978792428970337, 1.9570955038070679, 1.9637309312820435, 1.9816124439239502, 1.972067952156067, 1.9646470546722412, 1.9299079179763794, 1.9480232000350952, 1.9665381908416748, 1.9455419778823853, 1.9337157011032104, 1.9365986585617065, 1.9412058591842651, 1.9427223205566406, 1.9423376321792603, 1.9574627876281738, 1.9348481893539429, 1.9362848997116089, 1.9329547882080078, 1.9137599468231201, 1.9166059494018555, 1.926527976989746, 1.9153634309768677, 1.9064662456512451, 1.9150521755218506, 1.910338044166565, 1.9073282480239868, 1.9161107540130615, 1.91851007938385, 1.9101530313491821, 1.911741852760315, 1.9126914739608765, 1.9176486730575562, 1.9047771692276, 1.8968244791030884, 1.8934729099273682, 1.8999272584915161, 1.9038374423980713, 1.8957291841506958, 1.8913235664367676, 1.8996819257736206], 'val_acc': [0.2677305042743683, 0.2677305042743683, 0.2677305042743683, 0.2677305042743683, 0.2677305042743683, 0.2677305042743683, 0.2677305042743683, 0.2677305042743683, 0.2677305042743683, 0.2677305042743683, 0.2677305042743683, 0.2677305042743683, 0.2677305042743683, 0.3126477599143982, 0.332742303609848, 0.3481087386608124, 0.3676123023033142, 0.3888888955116272, 0.4231678545475006, 0.4556737542152405, 0.49763593077659607, 0.5070921778678894, 0.5236406326293945, 0.5206855535507202, 0.5348699688911438, 0.5443262457847595, 0.5460993051528931, 0.5431442260742188, 0.5413711667060852, 0.5413711667060852, 0.542553186416626, 0.542553186416626, 0.5460993051528931, 0.5478723645210266, 0.5496453642845154, 0.5531914830207825, 0.5591016411781311, 0.5579196214675903, 0.5620567202568054, 0.563829779624939, 0.5691489577293396, 0.5608747005462646, 0.5661938786506653, 0.567375898361206, 0.5726950168609619, 0.567375898361206, 0.5732860565185547, 0.5697399377822876, 0.5691489577293396, 0.5732860565185547, 0.5780141949653625, 0.5732860565185547, 0.5780141949653625, 0.5715129971504211, 0.5703309774398804, 0.5756500959396362, 0.5738770961761475, 0.5744680762290955, 0.5691489577293396, 0.5744680762290955, 0.5744680762290955, 0.5744680762290955, 0.5697399377822876, 0.5721040368080139, 0.5744680762290955, 0.5756500959396362, 0.5667848587036133, 0.5697399377822876, 0.5750591158866882, 0.5691489577293396, 0.5726950168609619, 0.5756500959396362, 0.5721040368080139, 0.5756500959396362, 0.567966878414154, 0.5721040368080139, 0.567966878414154, 0.5709219574928284, 0.5750591158866882, 0.5756500959396362, 0.5715129971504211, 0.5697399377822876, 0.5756500959396362, 0.5732860565185547, 0.5750591158866882, 0.5667848587036133, 0.5697399377822876, 0.5703309774398804, 0.5715129971504211, 0.5715129971504211, 0.5691489577293396, 0.563829779624939, 0.5709219574928284, 0.5709219574928284, 0.567966878414154, 0.5715129971504211, 0.5703309774398804, 0.5715129971504211, 0.5756500959396362, 0.5715129971504211], 'loss': [5.6652387454718305, 3.5464844233468162, 3.404894125487274, 3.3457786821765403, 3.3255700790897555, 3.3155429606311584, 3.309091754219287, 3.302657255240002, 3.2954878520559574, 3.285929946204027, 3.2714453827993935, 3.249048388692814, 3.2131732028513746, 3.1581851229672986, 3.0880262562459295, 3.0238698735630396, 2.95523032551364, 2.8834511132386895, 2.8078861112650237, 2.7315338945521757, 2.6589852220128263, 2.595763125490596, 2.5436391401778042, 2.501226771684221, 2.4679866407238147, 2.434841105990046, 2.403444895074305, 2.374521435511085, 2.350107498507585, 2.3239068286694335, 2.298953043711204, 2.277137438093681, 2.254976813804101, 2.2345680886244708, 2.21519594016295, 2.1978118066416052, 2.1799942857383696, 2.1624235361062674, 2.1444434808307733, 2.1310284534418678, 2.1167479665083264, 2.1040611305583257, 2.091516652008813, 2.0803148852845617, 2.0700027650625707, 2.0606841567521017, 2.050963199251317, 2.042100447500403, 2.030930707472604, 2.0254109183502855, 2.0192732171006886, 2.007195790465892, 1.998119005455584, 1.9914207957244887, 1.9837869609666172, 1.97761080682209, 1.9721684104818484, 1.9651034998792236, 1.9598684390198984, 1.9546963947318226, 1.9482948999645946, 1.9440082618004508, 1.9372811707787558, 1.9307865753742917, 1.9242632443029137, 1.91982445454408, 1.917977650569753, 1.9121337232826092, 1.907722473038475, 1.9028161363679583, 1.8967299823343766, 1.890132234026046, 1.8861985769178693, 1.8820657556362574, 1.87752451065671, 1.8729391308277072, 1.867506085981078, 1.8631928236867084, 1.857784705990881, 1.8556067715459663, 1.850118204261832, 1.8458457919863274, 1.8418025257398396, 1.8379805552434072, 1.834143683932421, 1.8312220534511925, 1.8267874518639957, 1.8224821669836202, 1.819675714924408, 1.8167488897015251, 1.8112921512827533, 1.807531287504873, 1.8035011658543727, 1.7995923507217793, 1.7953399394224292, 1.7916733319354832, 1.7879138175489064, 1.7842856985554687, 1.7815270285902232, 1.7764353873538534], 'acc': [0.2479176476783473, 0.2853253208668579, 0.2878211616134476, 0.28780325038423865, 0.2878450438058069, 0.2878450451119351, 0.28784504425796925, 0.2878510130133358, 0.28886606785974456, 0.2906215093918998, 0.29061553999450185, 0.2906812178868579, 0.29099170804932695, 0.29580425056287196, 0.3148215584543358, 0.327473887650325, 0.34727339240260024, 0.37090620146780334, 0.3886517090927724, 0.4032027937855742, 0.4174851783895875, 0.42938517477078614, 0.43772054856446574, 0.4433511068612628, 0.44845622664782586, 0.454630135606897, 0.4599382632835179, 0.46558673449361077, 0.4693185410909761, 0.4735399655425788, 0.47751658358171556, 0.4813976664206021, 0.48478316822495243, 0.48841347572909255, 0.4912138204210606, 0.49309465493736726, 0.49585918519132755, 0.4980505029906868, 0.49966264356110035, 0.5011434231753169, 0.5033287796233127, 0.5046125134132907, 0.5059201462215488, 0.5070128121076176, 0.5083025390683065, 0.5092399646776996, 0.5102609862147602, 0.5111208006882241, 0.5127687551499933, 0.5131389567746804, 0.5148287280339761, 0.5158377959488383, 0.517145429608393, 0.5175693583724493, 0.5193427108229408, 0.5196532080573645, 0.5197726326181977, 0.5206861806114848, 0.5213967134069801, 0.5220176918147212, 0.5228177895370376, 0.5234387619625691, 0.524829974068671, 0.5259107023212236, 0.5258450257323265, 0.5266272145268175, 0.527457172213415, 0.5277318424343431, 0.5280303832003123, 0.5281856276258134, 0.529308155851751, 0.5301261703518299, 0.5307829735882379, 0.5309202972770201, 0.5320547590732406, 0.5324727316794835, 0.5340968121141522, 0.5341684653339774, 0.5353984641108042, 0.535661183579635, 0.5359179517998439, 0.5363239479819862, 0.5373330393601784, 0.537607701061734, 0.5380256651724491, 0.5381570161415699, 0.5400677017009818, 0.5399662033928571, 0.5409872301022921, 0.5410887347908709, 0.5419246586753862, 0.5427247626827773, 0.5428083605109146, 0.5436681542948806, 0.5442174801415548, 0.5450235476666315, 0.5456684082428871, 0.5464565710119191, 0.547238759590027, 0.5475372960119632]}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "embedding_layer = Embedding(input_dim=NVOCAB+1, output_dim=EMBEDDING_DIM,\n",
    "                            input_length=N-1, weights=[E])\n",
    "\n",
    "\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(NHIDDEN1, return_sequences=True))\n",
    "model.add(LSTM(NHIDDEN2, return_sequences=True))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(NVOCAB))\n",
    "model.add(Activation(tf.nn.softmax))\n",
    "metrics = ['accuracy'] # loss is always the first metric returned from the fit method\n",
    "model.compile(loss=LOSS_FN, optimizer=OPTIMIZER, metrics=metrics)\n",
    "history = model.fit(x_train, y_train, batch_size=BATCH_SIZE, nb_epoch=NEPOCHS,\n",
    "                        validation_split=VALIDATION_SPLIT,)\n",
    "\n",
    "util.uprint('Final epoch generated text:', util.generate_text(model, data, N))\n",
    "print()\n",
    "\n",
    "print('history')\n",
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated text:\n",
      "ls ls parameter cd ls ls parameter cd ls cd filename ls ls filename ls cd filename ls cd ls\n",
      "filename ls cd filename ls cd ls cd ls ls cd cd ls cd filename ls emacs cd cd ls\n",
      "cd sfn sfn parameter emacs filename emacs filename ls parameter cd filename ls cd cd cd ls cd ls parameter\n",
      "average average filename basic fg basic basic average filename fg cd filename fg fg fg filename cd cd filename ls\n",
      "average parameter average filename cc parameter testrand fg average parameter cd filename fg fg cc parameter average filename sim fg\n",
      "parameter parameter more '' filename ls more parameter cd filename more cd filename ls more filename more parameter more filename\n",
      "fg f f filename tip filename logout f tip f f filename mail tip tip logout who who f f\n",
      "filename emacs filename cd ls emacs filename cd cd filename emacs emacs emacs emacs roff filename roff emacs filename cd\n",
      "parameter emacs cc average parameter cc filename emacs cc parameter /user/grads/xxxxxxx/connect/bin/makesim parameter emacs filename cc parameter emacs emacs sim sim\n",
      "parameter rlogin parameter rlogin filename who who filename who who filename rlogin who rlogin rlogin who rlogin who who rlogin\n",
      "Test loss: 2.5116543769836426\n",
      "Test accuracy: 0.5161645412445068\n"
     ]
    }
   ],
   "source": [
    "print('generated text:')\n",
    "nsentences = 10\n",
    "nwords_to_generate = 20\n",
    "k = 3\n",
    "for i in range(nsentences):\n",
    "    util.uprint(util.generate_text(model, data, N, nwords_to_generate, k))\n",
    "\n",
    "# calculate test accuracy on the heldout test data\n",
    "loss, accuracy = model.evaluate(x_test, y_test, BATCH_SIZE, verbose=0)\n",
    "print(\"Test loss:\",loss)\n",
    "print(\"Test accuracy:\",accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
